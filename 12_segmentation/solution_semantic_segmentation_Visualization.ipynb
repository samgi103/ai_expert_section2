{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"AYXIHngPoI2W"},"source":["# Image Segmentation using Fully Convolutional Network (FCN)\n","\n","---\n","## Instructions\n","- In this project, we will perform semantic segmentation on PASCAL VOC 2011 dataset which contains 20 object categories. We use the Semantic Boundaries Dataset(SBD) as it contains more segmentation labels than the original dataset.\n","- To this end, you need to implement necessary network components, load and fine-tune the pretrained network, and report segmentation performance on the validation set.\n","- Fill in the section marked **Px.x** with the appropriate code. Modify inside those areas, and not the skeleton code.\n","- To begin, you should download this ipynb file into your own Google drive clicking `make a copy(사본만들기)`. Find the copy in your drive, change their name to `semantic_segmentation.ipynb`, if their names were changed to e.g. `Copy of semantic_segmentation.ipynb` or `semantic_segmentation.ipynb의 사본`.\n","- You'll be training large models. We recommend you to create at least **6GB** of space available on your Google drive to run everything properly.\n"]},{"cell_type":"markdown","metadata":{"id":"x1ZGXqrvlc_O"},"source":["---\n","# Prerequisite: Mount your gdrive."]},{"cell_type":"code","metadata":{"id":"a0HEy2Tok-2u"},"source":["# mount drive https://datascience.stackexchange.com/questions/29480/uploading-images-folder-from-my-system-into-google-colab\n","# login with your google account and type authorization code to mount on your google drive.\n","import os\n","from google.colab import drive\n","drive.mount('/gdrive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_Tsq4xR-liMH"},"source":["---\n","# Prerequisite: Setup the `root` directory properly."]},{"cell_type":"code","metadata":{"id":"SHzfVbfmloDz"},"source":["# Specify the directory path where `semantic_segmentation.ipynb` exists.\n","# For example, if you saved `semantic_segmentation.ipynb` in `/gdrive/My Drive/cnn_practice/segmentation` directory,\n","# then set root = '/gdrive/My Drive/cnn_practice/segmentation'\n","root = '/gdrive/My Drive/samsung_vision/segmentation'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9XDVC3lJl1_H"},"source":["---\n","# Basic settings"]},{"cell_type":"markdown","metadata":{"id":"3jIiyjEGlwJ8"},"source":["## Import libraries"]},{"cell_type":"code","metadata":{"id":"eGlC_yM9lvue"},"source":["import os\n","import numpy as np\n","import time\n","from pathlib import Path\n","from PIL import Image\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","from torchvision.datasets import VOCSegmentation, SBDataset\n","from torchvision.datasets.vision import StandardTransform\n","from torchvision.utils import make_grid\n","from torchvision import transforms\n","from torchvision.models.vgg import VGG, vgg16, make_layers\n","from torch.optim import SGD\n","from torch.utils.tensorboard import SummaryWriter"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W27cdKyEmJy1"},"source":["## Set Hyperparameters"]},{"cell_type":"code","metadata":{"id":"dczwhJI1l8Kl"},"source":["# Basic settings\n","from easydict import EasyDict as edict\n","\n","cfg = {'vgg16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M']}\n","\n","torch.manual_seed(470)\n","torch.cuda.manual_seed(470)\n","\n","args = edict()\n","\n","args.batch_size = 1\n","args.lr = 1e-4\n","args.momentum = 0.9\n","args.weight_decay = 5e-4\n","args.epoch = 1\n","args.tensorboard = True\n","args.gpu = True\n","\n","device = 'cuda' if torch.cuda.is_available() and args.gpu else 'cpu'\n","\n","# Create directory name.\n","result_dir = Path(root) / 'results'\n","result_dir.mkdir(parents=True, exist_ok=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5iiGFJlxmCXW"},"source":["## Tensorboard"]},{"cell_type":"code","metadata":{"id":"2b3_oX_2mDw1"},"source":["# Setup tensorboard.\n","if args.tensorboard:\n","    %load_ext tensorboard\n","    %tensorboard --logdir \"/gdrive/My Drive/{str(result_dir).replace('/gdrive/My Drive/', '')}\" --samples_per_plugin images=100\n","else:\n","    writer = None"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XQZvsEeDmeZt"},"source":["---\n","# Utility functions\n","\n","Here are some utility functions that we will use throughout this assignment. You don't have to modify any of these."]},{"cell_type":"code","metadata":{"id":"LUOvr2A_mf4Q"},"source":["def get_parameters(model, bias=False):\n","    \"\"\" Extracts weight and bias parameters from a model.\"\"\"\n","    for m in model.modules():\n","        if isinstance(m, nn.Conv2d):\n","            if bias:\n","                yield m.bias\n","            else:\n","                yield m.weight\n","\n","# from https://github.com/shelhamer/fcn.berkeleyvision.org/blob/master/surgery.py\n","def get_upsampling_weight(in_channels, out_channels, kernel_size):\n","    \"\"\" Make a 2D bilinear kernel suitable for upsampling. \"\"\"\n","    factor = (kernel_size + 1) // 2\n","    if kernel_size % 2 == 1:\n","        center = factor - 1\n","    else:\n","        center = factor - 0.5\n","    og = np.ogrid[:kernel_size, :kernel_size]\n","    filt = (1 - abs(og[0] - center) / factor) * \\\n","           (1 - abs(og[1] - center) / factor)\n","    weight = np.zeros((in_channels, out_channels, kernel_size, kernel_size),\n","                      dtype=np.float64)\n","    weight[range(in_channels), range(out_channels), :, :] = filt\n","    return torch.from_numpy(weight).float()\n","\n","\n","class toLongTensor:\n","    \"\"\" Convert to Long Tensor from Byte Tensor \"\"\"\n","    def __call__(self, img):\n","        output = torch.from_numpy(np.array(img).astype(np.int32)).long()\n","        # To ignore boundaries.\n","        output[output == 255] = 21\n","        return output\n","\n","\n","def _fast_hist(label_true, label_pred, n_class):\n","    mask = (label_true >= 0) & (label_true < n_class)\n","    hist = np.bincount(\n","        n_class * label_true[mask].astype(int) +\n","        label_pred[mask], minlength=n_class ** 2).reshape(n_class, n_class)\n","    return hist\n","\n","def label_accuracy_score(label_trues, label_preds, n_class):\n","    \"\"\" Returns overall accuracy and mean IoU \"\"\"\n","    hist = np.zeros((n_class, n_class))\n","    for lt, lp in zip(label_trues, label_preds):\n","        hist += _fast_hist(lt.flatten(), lp.flatten(), n_class)\n","    acc = np.diag(hist).sum() / hist.sum()\n","    with np.errstate(divide='ignore', invalid='ignore'):\n","        iou = np.diag(hist) / (\n","            hist.sum(axis=1) + hist.sum(axis=0) - np.diag(hist)\n","        )\n","    mean_iou = np.nanmean(iou)\n","    return acc, mean_iou\n","\n","\n","class Colorize(object):\n","    \"\"\" Colorize the segmentation labels \"\"\"\n","    def __init__(self, n=35, cmap=None):\n","        if cmap is None:\n","            raise NotImplementedError()\n","            self.cmap = labelcolormap(n)\n","        else:\n","            self.cmap = cmap\n","        self.cmap = self.cmap[:n]\n","    def preprocess(self, x):\n","        if len(x.size()) > 3 and x.size(1) > 1:\n","            x = x.argmax(dim=1, keepdim=True).float()\n","        assert (len(x.shape) == 4) and (x.size(1) == 1), 'x should have a shape of [B, 1, H, W]'\n","        return x\n","    def __call__(self, x):\n","        x = self.preprocess(x)\n","        if (x.dtype == torch.float) and (x.max() < 2):\n","            x = x.mul(255).long()\n","        color_images = []\n","        gray_image_shape = x.shape[1:]\n","        for gray_image in x:\n","            color_image = torch.ByteTensor(3, *gray_image_shape[1:]).fill_(0)\n","            for label, cmap in enumerate(self.cmap):\n","                mask = (label == gray_image[0]).cpu()\n","                color_image[0][mask] = cmap[0]\n","                color_image[1][mask] = cmap[1]\n","                color_image[2][mask] = cmap[2]\n","            color_images.append(color_image)\n","        color_images = torch.stack(color_images)\n","        return color_images\n","\n","\n","def uint82bin(n, count=8):\n","    \"\"\" returns the binary of integer n, count refers to amount of bits \"\"\"\n","    return ''.join([str((n >> y) & 1) for y in range(count-1, -1, -1)])\n","\n","\n","def get_color_map():\n","    \"\"\" returns N color map \"\"\"\n","    N=25\n","    color_map = np.zeros((N, 3), dtype=np.uint8)\n","    for i in range(N):\n","        r, g, b = 0, 0, 0\n","        id = i\n","        for j in range(7):\n","            str_id = uint82bin(id)\n","            r = r ^ (np.uint8(str_id[-1]) << (7-j))\n","            g = g ^ (np.uint8(str_id[-2]) << (7-j))\n","            b = b ^ (np.uint8(str_id[-3]) << (7-j))\n","            id = id >> 3\n","        color_map[i, 0] = r\n","        color_map[i, 1] = g\n","        color_map[i, 2] = b\n","    color_map = torch.from_numpy(color_map)\n","    return color_map\n","\n","\n","def add_padding(img):\n","    \"\"\" Zero-pad image(or any array-like object) to 500x500.\"\"\"\n","    w, h = img.shape[-2], img.shape[-1]\n","    MAX_SIZE = 500\n","    IGNORE_IDX = 21\n","\n","    assert max(w, h) <= MAX_SIZE, f'both height and width should be less than {MAX_SIZE}'\n","\n","    _pad_left = (MAX_SIZE - w) // 2\n","    _pad_right = (MAX_SIZE - w + 1) // 2\n","    _pad_up = (MAX_SIZE - h) // 2\n","    _pad_down = (MAX_SIZE - h + 1) // 2\n","\n","    _pad = (_pad_up, _pad_down, _pad_left, _pad_right)\n","\n","    padding_img = transforms.Pad(_pad)\n","    padding_target = transforms.Pad(_pad, fill=IGNORE_IDX)\n","\n","    img = F.pad(img, pad=_pad)\n","    return img\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r7WMaIEVmoub"},"source":["---\n","# Define `DataLoader` for training & validation set\n","\n","If the cell below fails with error message \"Destination path `./cls` already exists\", try again with `download=False`."]},{"cell_type":"code","metadata":{"id":"do8qfYkEmtNg"},"source":["mean = [.485, .456, .406]\n","std = [.229, .224, .225]\n","\n","# Define transform functions.\n","transform_train = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize([.485, .456, .406], [.229, .224, .225])\n","])\n","transform_train_target = transforms.Compose([\n","    toLongTensor()\n","])\n","transform_test = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize([.485, .456, .406], [.229, .224, .225])\n","])\n","transform_test_target = transforms.Compose([\n","    toLongTensor()\n","])\n","\n","# Define dataloader.\n","sbd_transform_train = StandardTransform(transform_train, transform_train_target)\n","sbd_transform_test = StandardTransform(transform_test, transform_test_target)\n","train_dataset = SBDataset(root='.', image_set='train', mode='segmentation', download=True, transforms=sbd_transform_train)\n","test_dataset = SBDataset(root='.', image_set='val', mode='segmentation', download=False, transforms=sbd_transform_test)\n","train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OVy7Dc_rmw2A"},"source":["---\n","# Define networks"]},{"cell_type":"markdown","metadata":{"id":"Sht44By2jhcs"},"source":["## P1. Implement FCN32 [(Illustration)](https://docs.google.com/drawings/d/1Vlm1kdIH9MZiJU_gLFhVNO_-alFGWyxLF_zSmQxkaO0/edit?usp=sharing)\n","\n","### (a) Declare convolutional layers as an alternative to FC layers\n","The VGG16 backbone contains two fully connected layers (fc6, fc7). In fully convolutional network, these fc layers will be convolutionized. You should define proper convolutional layers, which will be initialized from fc layers of pretrained model in section (d).\n","\n","### (b) Declare pixel-wise classification layer using 1x1 convolution\n","This section declares 1x1 convolution layer that produces classification per pixel\n","\n","### (c) Declare a deconvolution layer\n","Declare a learnable upsampling layer.\n","\n","### (d) Load pretrained weights\n","As mentioned in section (a), you should convolutionize the two fc layers(fc6, fc7) of pretrained model inside `load_pretrained` method. Initialize two convolutional layers with fc6, fc7 layers of the pretrained model, respectively.\n","\n","### (e) Implement `forward` method\n","The `forward` method should\n","- Feed input through the backbone to get `pool5` (given)\n","- On top of `pool5`, perform pixel-wise classification.\n","- Upsample the prediction to create segmentation on the original image resolution level."]},{"cell_type":"code","metadata":{"id":"NLC1mpEFm05u"},"source":["class FCN32(VGG):\n","    def __init__(self):\n","        super(FCN32, self).__init__(make_layers(cfg['vgg16']))\n","\n","        self.numclass = 21\n","\n","        self.relu = nn.ReLU(True)\n","        self.dropout = nn.Dropout2d()\n","\n","        # fc layers in vgg are all converted into conv layers.\n","        #################################\n","        ## P1(a). Write your code here ##\n","        self.conv6 = nn.Conv2d(512, 4096, kernel_size=7)\n","        self.conv7 = nn.Conv2d(4096, 4096, kernel_size=1)\n","        #################################\n","\n","        # Prediction layer with 1x1 convolution layer.\n","        #################################\n","        ## P1(b). Write your code here ##\n","        self.score_x32 = nn.Conv2d(4096, self.numclass, kernel_size=1)\n","        #################################\n","\n","        # Learnable upsampling layers in FCN model.\n","        #################################\n","        ## P1(c). Write your code here ##\n","        self.deconv1 = nn.ConvTranspose2d(self.numclass, self.numclass, kernel_size=64, stride=32, bias=False)\n","        #################################\n","\n","        # initialize deconv layer with bilinear upsampling.\n","        self._initialize_weights()\n","\n","    def load_pretrained(self, pretrained_model):\n","        self.features = pretrained_model.features\n","        fc6 = pretrained_model.classifier[0]\n","        fc7 = pretrained_model.classifier[3]\n","\n","        # Convolutionize the weights.\n","        #################################\n","        ## P1(d) ##\n","        self.conv6.weight.data = fc6.weight.data.view(self.conv6.weight.data.shape)\n","        self.conv6.bias.data = fc6.bias.data.view(self.conv6.bias.data.shape)\n","        self.conv7.weight.data = fc7.weight.data.view(self.conv7.weight.data.shape)\n","        self.conv7.bias.data = fc7.bias.data.view(self.conv7.bias.data.shape)\n","        #################################\n","\n","    def vgg_layer_forward(self, x, indices):\n","        output = x\n","        start_idx, end_idx = indices\n","        for idx in range(start_idx, end_idx):\n","            output = self.features[idx](output)\n","        return output\n","\n","    def vgg_forward(self, x):\n","        out = {}\n","        layer_indices = [0, 5, 10, 17, 24, 31]\n","        for layer_num in range(len(layer_indices)-1):\n","            x = self.vgg_layer_forward(x, layer_indices[layer_num:layer_num+2])\n","            out[f'pool{layer_num+1}'] = x\n","        return out\n","\n","    def forward(self, x):\n","        # Padding for aligning to the input size\n","        padded_x = F.pad(x, [100, 100, 100, 100], \"constant\", 0)\n","        vgg_features = self.vgg_forward(padded_x)\n","        vgg_pool5 = vgg_features['pool5'].detach()\n","\n","        # prediction at stride 32.\n","        out = self.dropout(self.relu(self.conv6(vgg_pool5)))\n","        out = self.dropout(self.relu(self.conv7(out)))\n","        #################################\n","        ## P1(e). Write your code here ##\n","        pred_x32 = self.score_x32(out)\n","        #################################\n","\n","        # upsample with conv transpose\n","        #################################\n","        ## P1(e). Write your code here ##.\n","        out = self.deconv1(pred_x32)\n","        #################################\n","        out = out[..., 9:9+x.shape[-2], 9:9+x.shape[-1]]\n","\n","        return out\n","\n","    # initialize transdeconv layer with bilinear upsampling.\n","    def _initialize_weights(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.ConvTranspose2d):\n","                assert m.kernel_size[0] == m.kernel_size[1]\n","                initial_weight = get_upsampling_weight(\n","                    m.in_channels, m.out_channels, m.kernel_size[0])\n","                m.weight.data.copy_(initial_weight)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TG_fK-sWjLT5"},"source":["## P2. Implement FCN8 [(Illustration)](https://docs.google.com/drawings/d/1C5bbCgm6Wm_FEw_colmNvzI-feSZBffqtCcXP4hgRj0/edit?usp=sharing)\n","This section **builds on top of** P1. Make sure you finished your implementation of FCN32 first.\n","\n","### (a) Declare convolutional layers as an alternative to FC layers\n","The VGG16 backbone contains two fully connected layers (fc6, fc7). In fully convolutional network, these fc layers will be convolutionized. You should define proper convolutional layers.\n","**Note**: If a layer inherited from `FCN32` is used the same way in `FCN8`, then there's no need to re-declare those layers.\n","\n","### (b) Declare pixel-wise classification layer using 1x1 convolution\n","This section declares **three** 1x1 convolution layers that produce classification per pixel on different resolutions. Details are provided in illustration.\n","\n","### (c) Declare deconvolution layers\n","Create **three** learnable upsampling layers. Details are provided in illustration.\n","\n","### (d) Implement skip connection and `forward` method\n","The `forward` method should\n","- Feed input through the backbone to get `pool5` (given)\n","- perform pixel-wise classification on three levels (`vgg_pool[3~5]`).\n","- Upsample the prediction of `pool5`, add a skip connection from the prediction of `pool4`.\n","- Upsample the prediction of `pool4` and `pool5` combined, add a skip connection from the prediction of `pool3`.\n","- Upsample the prediction of `pool[3~5]` combined to create segmentation on the input image resolution level."]},{"cell_type":"code","metadata":{"id":"15ONw36JnG1j"},"source":["class FCN8(FCN32):\n","    def __init__(self):\n","        super(FCN8, self).__init__()\n","\n","        self.numclass = 21\n","\n","        self.relu = nn.ReLU(True)\n","        self.dropout = nn.Dropout2d()\n","\n","        # fc layers in vgg are all converted into conv layers.\n","        #################################\n","        ## P2(a). Write your code here ##\n","        self.conv6 = nn.Conv2d(512, 4096, kernel_size=7)\n","        self.conv7 = nn.Conv2d(4096, 4096, kernel_size=1)\n","        #################################\n","\n","        # prediction layers with 1x1 convolution layers.\n","        #################################\n","        ## P2(b). Write your code here ##\n","        self.score_x32 = nn.Conv2d(4096, self.numclass, kernel_size=1)\n","        self.score_x16 = nn.Conv2d(512, self.numclass, kernel_size=1)\n","        self.score_x8 = nn.Conv2d(256, self.numclass, kernel_size=1)\n","        #################################\n","\n","        # Learnable upsampling layers in FCN model.\n","        #################################\n","        ## P2(c). Write your code here ##\n","        self.deconv1 = nn.ConvTranspose2d(self.numclass, self.numclass, kernel_size=4, stride=2, bias=False)\n","        self.deconv2 = nn.ConvTranspose2d(self.numclass, self.numclass, kernel_size=4, stride=2, bias=False)\n","        self.deconv3 = nn.ConvTranspose2d(self.numclass, self.numclass, kernel_size=16, stride=8, bias=False)\n","        #################################\n","\n","        # initialize deconv layer with bilinear upsampling.\n","        self._initialize_weights()\n","\n","    def forward(self, x):\n","        # Padding for aligning to the input size\n","        padded_x = F.pad(x, [100, 100, 100, 100], \"constant\", 0)\n","        vgg_features = self.vgg_forward(padded_x)\n","        vgg_pool5 = vgg_features['pool5'].detach()\n","        vgg_pool4 = vgg_features['pool4'].detach()\n","        vgg_pool3 = vgg_features['pool3'].detach()\n","\n","        # prediction at stride 32.\n","        out = self.dropout(self.relu(self.conv6(vgg_pool5)))\n","        out = self.dropout(self.relu(self.conv7(out)))\n","\n","        #################################\n","        ## P2(d). Write your code here ##\n","        pred_x32 = self.score_x32(out)\n","        pred_x16 = self.score_x16(vgg_pool4 * 0.01) # For all at once training, we should compensate scaling\n","        pred_x8 = self.score_x8(vgg_pool3 * 0.0001) # For all at once training, we should compensate scaling\n","        #################################\n","\n","        # upsample.\n","        #################################\n","        ## P2(d). Write your code here ##\n","        up_pred_x32 = self.deconv1(pred_x32)\n","        #################################\n","\n","        # sum two predictions from different layer.\n","        out = up_pred_x32 + pred_x16[..., 5:5 + up_pred_x32.shape[-2], 5:5 + up_pred_x32.shape[-1]]\n","\n","        # upsample.\n","        #################################\n","        ## P2(d). Write your code here ##\n","        up_pred_x16 = self.deconv2(out)\n","        #################################\n","\n","        # sum two predictions from different layer.\n","        out = up_pred_x16 + pred_x8[..., 9:9 + up_pred_x16.shape[-2], 9:9 + up_pred_x16.shape[-1]]\n","\n","        # final upsampling to original resolution.\n","        #################################\n","        ## P2(d). Write your code here ##\n","        out = self.deconv3(out)\n","        #################################\n","        out = out[..., 31:31+x.shape[-2], 31:31+x.shape[-1]]\n","\n","        return out\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Fz3cbKImnMnh"},"source":["---\n","# Training function"]},{"cell_type":"markdown","metadata":{"id":"lmlQMp_XjlPe"},"source":["## P3. Implement training pipeline\n","\n","This section contains 2 problems.\n","### (a) Forward/Backward step for training\n","\n","- Feed the image through the model.\n","- Perform a gradient step based on the loss. Loss can be calculated using `criterion`, located at the beginning of the function.\n","- Choose the highest logit per pixel as prediction.\n","\n","### (b) Forward step for validation\n","- Feed the image through the model.\n","- Calculate loss on current image.\n","- Choose the highest logit per pixel as prediction."]},{"cell_type":"code","metadata":{"id":"qrGDOpkAnQMh"},"source":["def train_net(net, resume=False):\n","    # 21 is the index for boundaries: therefore we ignore this index.\n","    criterion = nn.CrossEntropyLoss(ignore_index=21)\n","    colorize = Colorize(21, get_color_map())\n","    best_valid_iou = 0\n","\n","    if resume:\n","        checkpoint = torch.load(ckpt_path, map_location=device)\n","        net.load_state_dict(checkpoint['state_dict'])\n","        optimizer.load_state_dict(checkpoint['optimizer'])\n","        epoch = checkpoint['epoch']\n","        best_valid_iou = checkpoint['best_valid_iou']\n","        print(f'Resume training from epoch {epoch+1}')\n","    else:\n","        epoch = 0\n","\n","    while epoch < args.epoch:\n","        t1 = time.time()\n","        saved_images, saved_labels = [], []\n","\n","        # Start training\n","        net.train()\n","\n","        loss_total = 0\n","        ious = []\n","        pixel_accs = []\n","\n","        for batch_idx, (image, label) in enumerate(train_loader):\n","            # Save images for visualization.\n","            if len(saved_images) < 4:\n","                saved_images.append(image.cpu())\n","                saved_labels.append(add_padding(label.cpu()))\n","\n","            # Move variables to gpu.\n","            image = image.to(device)\n","            label = label.to(device)\n","\n","            # Feed foward.\n","            output = net(image)\n","\n","            # Calculate loss.\n","            loss = criterion(output, label)\n","\n","            # Perform gradient descent step.\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            # pred: choose the label with the highest logit in each pixel.\n","            pred = torch.argmax(output, dim=1)\n","\n","            target = label.squeeze(1).cpu().numpy()\n","\n","            # calculate pixel accuarcy and mean IoU\n","            acc, mean_iu = label_accuracy_score(target, pred.cpu().detach().numpy(), n_class=21)\n","\n","            pixel_accs.append(acc)\n","            ious.append(mean_iu)\n","\n","            # Update total loss.\n","            loss_total += loss.item()\n","\n","            if batch_idx % 50 == 0:\n","                print(f'Epoch : {epoch} || {batch_idx}/{len(train_loader)} || \\\n","                 loss : {loss.item():.3f}, iou : {mean_iu * 100:.3f} pixel_acc : {acc * 100:.3f}')\n","\n","        # Calculate average IoU\n","        total_ious = np.array(ious).T\n","        total_ious = np.nanmean(total_ious).mean()\n","        total_pixel_acc = np.array(pixel_accs).mean()\n","\n","        writer.add_scalar('train_loss', loss_total / len(train_loader), epoch)\n","        writer.add_scalar('pixel_acc', total_pixel_acc, epoch)\n","        writer.add_scalar('mean_iou', total_ious, epoch)\n","\n","        # Image visualization\n","        un_norms, preds, outputs = [], [], []\n","        for image, label in zip(saved_images, saved_labels):\n","            # Denormalize the image.\n","            image_permuted = image.permute(1, 0, 2, 3)\n","            un_norm = torch.zeros_like(image_permuted)\n","            for idx, (im, m, s) in enumerate(zip(image_permuted, mean, std)):\n","                un_norm[idx] = (im * s) + m\n","            un_norm = un_norm.permute(1, 0, 2, 3)\n","            un_norms.append(add_padding(un_norm))\n","\n","            with torch.no_grad():\n","                output = net(image.to(device))\n","                pred = torch.argmax(output, dim=1)\n","                preds.append(add_padding(pred))\n","\n","        # Stitch images into a grid.\n","        un_norm = make_grid(torch.cat(un_norms), nrow=2)\n","        label = make_grid(colorize(torch.stack(saved_labels)), nrow=2)\n","        pred = make_grid(colorize(torch.stack(preds)), nrow=2)\n","\n","        # Write images to Tensorboard.\n","        writer.add_image('img', un_norm, epoch)\n","        writer.add_image('gt', label, epoch)\n","        writer.add_image('pred', pred, epoch)\n","\n","        t = time.time() - t1\n","        print(f'>> Epoch : {epoch} || AVG loss : {loss_total / len(train_loader):.3f}, \\\n","         iou : {total_ious * 100:.3f} pixel_acc : {total_pixel_acc * 100:.3f} {t:.3f} secs')\n","\n","        # Evaluation\n","        net.eval()\n","        saved_images, saved_labels = [], []\n","\n","        valid_loss_total = 0\n","        valid_ious = []\n","        valid_pixel_accs = []\n","\n","        with torch.no_grad():\n","            for batch_idx, (image, label) in enumerate(test_loader):\n","                # Save images for visualization.\n","                if len(saved_images) < 4:\n","                    saved_images.append(image.cpu())\n","                    saved_labels.append(add_padding(label.cpu()))\n","\n","                # Move variables to gpu.\n","                image = image.to(device)\n","                label = label.to(device)\n","\n","                # Feed foward.\n","                output = net(image)\n","\n","                # Calculate loss.\n","                loss = criterion(output, label)\n","\n","                # pred: choose the label with the highest logit in each pixel.\n","                pred = torch.argmax(output, dim=1)\n","\n","                target = label.squeeze(1).cpu().numpy()\n","                acc, mean_iu = \\\n","                    label_accuracy_score(target, pred.cpu().numpy(), n_class=21)\n","\n","                # Update total loss.\n","                valid_loss_total += loss.item()\n","\n","                valid_pixel_accs.append(acc)\n","                valid_ious.append(mean_iu)\n","\n","        # Calculate average IoU\n","        total_valid_ious = np.array(valid_ious).T\n","        total_valid_ious = np.nanmean(total_valid_ious).mean()\n","        total_valid_pixel_acc = np.array(valid_pixel_accs).mean()\n","\n","        writer.add_scalar('valid_train_loss', valid_loss_total / len(test_loader), epoch)\n","        writer.add_scalar('valid_pixel_acc', total_valid_pixel_acc, epoch)\n","        writer.add_scalar('valid_mean_iou', total_valid_ious, epoch)\n","\n","        # Image visualization\n","        un_norms, preds, outputs = [], [], []\n","        for image, label in zip(saved_images, saved_labels):\n","            # Denormalize the image.\n","            image_permuted = image.permute(1, 0, 2, 3)\n","            un_norm = torch.zeros_like(image_permuted)\n","            for idx, (im, m, s) in enumerate(zip(image_permuted, mean, std)):\n","                un_norm[idx] = (im * s) + m\n","            un_norm = un_norm.permute(1, 0, 2, 3)\n","            un_norms.append(add_padding(un_norm))\n","\n","            with torch.no_grad():\n","                output = net(image.to(device))\n","                outputs.append(add_padding(output))\n","                pred = torch.argmax(output, dim=1)\n","                preds.append(add_padding(pred))\n","\n","        # Stitch images into a grid.\n","        valid_un_norm = make_grid(torch.cat(un_norms), nrow=2)\n","        valid_label = make_grid(colorize(torch.stack(saved_labels)), nrow=2)\n","        valid_pred = make_grid(colorize(torch.stack(preds)), nrow=2)\n","\n","        # Write images to tensorboard.\n","        writer.add_image('valid_img', valid_un_norm, epoch)\n","        writer.add_image('valid_gt', valid_label, epoch)\n","        writer.add_image('valid_pred', valid_pred, epoch)\n","\n","        print(f'>> Epoch : {epoch} || AVG valid loss : {valid_loss_total / len(test_loader):.3f}, \\\n","          iou : {total_valid_ious * 100:.3f} pixel_acc : {total_valid_pixel_acc * 100:.3f} {t:.3f} secs')\n","\n","        # Save checkpoints every epoch.\n","        checkpoint = {\n","            'epoch': epoch + 1,\n","            'state_dict': net.state_dict(),\n","            'optimizer': optimizer.state_dict(),\n","            'best_valid_iou': best_valid_iou\n","        }\n","        torch.save(checkpoint, ckpt_path)\n","\n","        # Save best checkpoint.\n","        if total_valid_ious > best_valid_iou:\n","            best_valid_iou = total_valid_ious\n","            torch.save(net.state_dict(), ckpt_dir / 'best.pt')\n","\n","        epoch += 1\n","    print(f'>> Best validation set iou: {best_valid_iou}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k0758O0Fnbk2"},"source":["---\n","# Train models through the pipeline\n","\n","In this section, you will\n","- Create/load directory.\n","- Select which model to train.\n","- Create model and optimizer.\n","\n","The training process will automatically save checkpoints to your Google drive after every epoch under `parent_dir`. Training could take up to 40 minutes per epoch. As we provide  pretrained weights to start with, you will only be training for 10 epochs on your own. Uncomment the lines after `# Select model here.` to choose which model to train.  \n","**You must load the provided pretrained weights**, otherwise achieving reasonable performance will take much longer.  \n","**If you would like to resume** from an existing `model.pt`, then\n","- Comment out the line below `Load pretrained weights here.`,\n","- Specify `parent_dir` as instructed,\n","- Run the first code cell again, then run `train_net` with `resume=True` parameter.  \n","\n","<font color=\"red\">Do not terminate your process right after an epoch has finished.</font> Writing the saved model back to Google drive will take an extra couple of minutes, and aborting in the middle will likely ruin your checkpoint file."]},{"cell_type":"code","metadata":{"id":"xAs5ugisnfAA"},"source":["num_trial=0\n","result_dir= Path(root) / 'results'\n","parent_dir = result_dir / f'trial_{num_trial}'\n","while parent_dir.is_dir():\n","    num_trial = int(parent_dir.name.replace('trial_',''))\n","    parent_dir = result_dir / f'trial_{num_trial+1}'\n","\n","# Modify parent_dir here if you want to resume from a checkpoint, or to rename directory.\n","# parent_dir = result_dir / 'trial_99'\n","print(f'Logs and ckpts will be saved in : {parent_dir}')\n","\n","log_dir = parent_dir\n","ckpt_dir = parent_dir\n","ckpt_path = parent_dir / 'model.pt'\n","writer = SummaryWriter(log_dir)\n","\n","# Load pretrained weights.\n","pretrained_path = Path(root) / 'pretrained_vgg.pt'\n","pretrained_VGG = vgg16().to(device)\n","pretrained_VGG.load_state_dict(torch.load(pretrained_path, map_location=device))\n","\n","# Select model here.\n","model = FCN8().to(device)\n","# model = FCN32().to(device)\n","\n","# Load pretrained weights here.\n","model.load_pretrained(pretrained_VGG)\n","\n","# Define optimizer.\n","# According to FCN paper, we doubled the learning rate of bias compared to that of weight.\n","optimizer = SGD([{'params': get_parameters(model, True), 'lr': args.lr * 2, 'weight_decay': 0},\n","                 {'params': get_parameters(model, False)}],\n","                lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F5UErfucnhSy"},"source":["train_net(model, resume=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ybgRjHkWLAeg"},"source":["# P4. Aggregating results\n"]},{"cell_type":"markdown","metadata":{"id":"1FfRElMMIKm0"},"source":["After you've trained both FCN32 and FCN8, load your best models and run the following block to check validation accuracy. Since the validation set contains nearly 3,000 images, this will take up to 30 minutes."]},{"cell_type":"code","metadata":{"id":"7FupQB9oK9zC"},"source":["# Specify path to your best trained model.\n","# For example if you want to FCN32 load from folder 'trial_5', modify 'trial_99' into 'trial_5'.\n","FCN32_path = result_dir / 'trial_99' / 'best.pt'\n","FCN8_path = result_dir / 'trial_98' / 'best.pt'\n","\n","model1 = FCN32().to(device)\n","model1.load_state_dict(torch.load(FCN32_path, map_location=device))\n","model2 = FCN8().to(device)\n","model2.load_state_dict(torch.load(FCN8_path, map_location=device))\n","\n","criterion = nn.CrossEntropyLoss(ignore_index=21)\n","colorize = Colorize(21, get_color_map())\n","\n","for net in [model1, model2]:\n","    net.eval()\n","\n","    valid_loss_total = 0\n","    valid_ious = []\n","    valid_pixel_accs = []\n","\n","    with torch.no_grad():\n","        for batch_idx, (image, label) in enumerate(test_loader):\n","            # Move variables to gpu.\n","            image = image.to(device)\n","            label = label.to(device)\n","\n","            # Feed foward.\n","            output = net(image)\n","\n","            # Calculate loss.\n","            loss = criterion(output, label)\n","            pred = torch.argmax(output, dim=1)\n","\n","            target = label.squeeze(1).cpu().numpy()\n","            acc, mean_iu = \\\n","                label_accuracy_score(target, pred.cpu().numpy(), n_class=21)\n","\n","            # Update total loss.\n","            valid_loss_total += loss.item()\n","\n","            valid_pixel_accs.append(acc)\n","            valid_ious.append(mean_iu)\n","\n","        # Calculate average IoU\n","        total_valid_ious = np.array(valid_ious).T\n","        total_valid_ious = np.nanmean(total_valid_ious).mean()\n","        total_valid_pixel_acc = np.array(valid_pixel_accs).mean()\n","\n","        print(f'{type(net).__name__}:')\n","        print(f'Pixel accuracy: {total_valid_pixel_acc * 100:.3f}, mIoU: {total_valid_ious:.3f}')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Visualization"],"metadata":{"id":"WUdXXU-J2ahs"}},{"cell_type":"code","source":["from torchvision.utils import make_grid\n","from torchvision.transforms import ToPILImage\n","\n","\n","mean = [.485, .456, .406]\n","std = [.229, .224, .225]\n","\n","def unnormalize(x):\n","    m = torch.tensor(mean)[None, :, None, None].to(x.device)\n","    s = torch.tensor(std)[None, :, None, None].to(x.device)\n","    return (x*s + m)\n","\n","topil = ToPILImage()\n","\n","vis_idx = 0\n","vis = []\n","with torch.no_grad():\n","    for batch_idx, (image, label) in enumerate(test_loader):\n","        if batch_idx == vis_idx:\n","            image = image.to(device)\n","            label = label.to(device)\n","            vis.append(unnormalize(image))\n","            vis.append(label[:, None].repeat(1, 3, 1, 1))\n","\n","            for net in [model1, model2]:\n","                output = net(image)\n","                loss = criterion(output, label)\n","                pred = torch.argmax(output, dim=1)\n","                vis.append(pred[:, None].repeat(1, 3, 1, 1))\n","\n","            break\n","\n","vis = torch.cat(vis, dim=0)\n","vis = make_grid(vis, nrow=4, padding=1, pad_value=1.0)\n","topil(vis).show()"],"metadata":{"id":"6H_ECVw82Zxv"},"execution_count":null,"outputs":[]}]}