{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python (reco)","language":"python","name":"reco_base"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.11"},"colab":{"provenance":[]}},"cells":[{"cell_type":"markdown","source":["# AI 전문가 교육과정 실습 2 - part Final\n","\n","***\n","### NLP응용: 토픽 추출\n","Applied Natrual Language Processing: Topic Modeling\n","\n","강사: 차미영 교수 (카이스트 전산학부)    \n","조교: 신민기, 정현규 (카이스트 전산학부)\n","\n","실습 담당: 신민기 (mingi.shin@kaist.ac.kr)\n","\n","# LDA on Korean Data\n","\n","한국어 데이터를 처리하기 위해 konlpy, openjdk, JPype, mecab 등을 설치한다."],"metadata":{"id":"Gp-fkNdjA-03"}},{"cell_type":"code","metadata":{"id":"7VJBYlQjMFz6"},"source":["!pip install pyLDAvis==3.2.2\n","!pip install konlpy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zR9ObV64NzUe"},"source":["!apt-get update\n","!apt-get install g++ openjdk-8-jdk\n","!pip3 install konlpy JPype1-py3\n","!bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)\n","!pip3 install mecab-python"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2zexZe0jL5Qt"},"source":["import glob\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","import re\n","from konlpy.tag import Mecab\n","from wordcloud import WordCloud\n","\n","import gensim\n","import gensim.corpora as corpora\n","from gensim.utils import simple_preprocess\n","from gensim.models import CoherenceModel\n","from pprint import pprint\n","\n","import pyLDAvis\n","import pyLDAvis.gensim\n","%matplotlib inline\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","import warnings\n","warnings.filterwarnings(action='ignore')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4vnXb_fTL-fC"},"source":["import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SxnpzMVWL5Qx"},"source":["# Data load\n","\n","데이터: 코로나 바이러스와 관련된 키워드를 언급한 한국어 트위터 데이터\n","\n","먼저 zip 파일을 다운 받고, \"세션 저장소에 업로드\" 기능을 이용해 업로드해주세요."]},{"cell_type":"code","source":["# 압축 해제\n","!unzip ./data_topic.zip"],"metadata":{"id":"yOvEB0vSHqNC"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Sw0v4Yx3L5Qx"},"source":["file_list = glob.glob(\"./data/*.json\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qLkF92OgL5Qz"},"source":["total_df = pd.read_json(file_list[0], lines=True)\n","total_df.reset_index(inplace = True)\n","original_df = total_df.copy()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dPGRklFBL5Qz"},"source":["total_df.head(3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PhngsdXaL5Q0"},"source":["total_df.describe()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x1_ouHkmL5Q1"},"source":["# Pre-processing function"]},{"cell_type":"code","metadata":{"id":"UMjT93lyL5Q1"},"source":["# Basic Cleaning Text Function\n","def CleanText(readData):\n","\n","    # Remove Retweets\n","    text = re.sub('RT @[\\w_]+: ', '', readData)\n","\n","    # Remove Mentions\n","    text = re.sub('@[\\w_]+', '', text)\n","\n","    # Remove or Replace URL\n","    # text = url_re.sub('URL', text)\n","    text = re.sub(r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\", ' ', text) # start with http\n","    text = re.sub(r\"[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{2,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)\", ' ', text) # Don't start with http\n","\n","    # Remove Hashtag\n","    text = re.sub('[#]+[0-9a-zA-Z_]+', ' ', text)\n","\n","    # Remove Garbage Words (ex. &lt, &gt, etc)\n","    text = re.sub('[&]+[a-z]+', ' ', text)\n","\n","    # Remove Special Characters\n","    text = re.sub('[^0-9a-zA-Zㄱ-ㅎ가-힣]', ' ', text)\n","\n","    # Remove Numbers (If you want, activate the code)\n","    text = re.sub(r'\\d+',' ',text)\n","\n","    # Remove English (If you want, activate the code)\n","    text = re.sub('[a-zA-Z]' , ' ', text)\n","\n","    # Remove newline\n","    text = text.replace('\\n',' ')\n","\n","    # Remove multi spacing & Reform sentence\n","    text = ' '.join(text.split())\n","\n","    return text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WjCi6KqYL5Q2"},"source":["def preprocessing_mecab(readData):\n","    #### Clean text\n","    sentence = CleanText(readData)\n","\n","    #### Tokenize\n","    morphs = mecab.pos(sentence)\n","\n","    # refer https://konlpy-ko.readthedocs.io/ko/v0.4.3/morph/ for more details\n","    JOSA = [\"JKS\", \"JKC\", \"JKG\", \"JKO\", \"JKB\", \"JKV\", \"JKQ\", \"JX\", \"JC\"]\n","    SIGN = [\"SF\", \"SE\", \"SSO\", \"SSC\", \"SC\", \"SY\"]\n","    TERMINATION = [\"EP\", \"EF\", \"EC\", \"ETN\", \"ETM\"] # 어미\n","    SUPPORT_VERB = [\"VX\"]\n","    NUMBER = [\"SN\"]\n","\n","    # Remove JOSA, EOMI, etc\n","    morphs[:] = (morph for morph in morphs if morph[1] not in JOSA+SIGN+TERMINATION+SUPPORT_VERB)\n","\n","    # If you want to save only Nouns:\n","    # morphs = mecab.nouns(sentence)\n","\n","    # Remove Stopwords\n","    morphs[:] = (morph for morph in morphs if morph[0] not in korean_stopwords[\"형태\"].tolist())\n","    morphs[:] = (morph for morph in morphs if morph[0] not in my_korean_stopwords[\"형태\"].tolist())\n","\n","    # Remove length-1 words\n","    morphs[:] = (morph for morph in morphs if not (len(morph[0]) == 1))\n","\n","    # Remove Numbers\n","    morphs[:] = (morph for morph in morphs if morph[1] not in NUMBER)\n","\n","    # Result pop-up\n","    result = []\n","    for morph in morphs:\n","        result.append(morph[0])\n","\n","    return result"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fp-dDQXhL5Q3"},"source":["# Korean Stopwords Load\n","korean_stopwords = pd.read_csv(\"./data/korean_stopwords.txt\", delimiter='\\t', names=[\"형태\", \"품사\", \"비율\"])\n","\n","# Add Custom Korean Stopwords\n","my_data = [[\"님\", \"NNG\"], [\"들\", \"XSN\"], [\"ㅋㅋㄱㅋㄱㅋ\", \"NNG\"],\n","           [\"오늘\", \"NNG\"], [\"얘기\", \"NNG\"], [\"ㅠㅠ\", \"NNG\"], [\"없이\", ], [\"딱히\", ],\n","           ['ㅋㅋ', ], ['ㅋㅋㅋ', ], [\"그런데\", ], [\"누구\", ], [\"여기저기\", ]]\n","\n","my_korean_stopwords = pd.DataFrame(my_data, columns = ['형태', '품사'])\n","\n","# Main\n","mecab = Mecab(dicpath=\"/usr/local/lib/mecab/dic/mecab-ko-dic\") # Mecab Dictionary Path"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wWRXiATSL5Q3"},"source":["SAMPLE_TEXT = \"RT @boxplus01: 美언론, 'abc한국 코로나 확산주범은 신천지와 보수세력' https://t.co/Phq0l48aUm\"\n","print(\"Before preprocessing : {}\".format(SAMPLE_TEXT))\n","print(\"After preprocessing : {}\".format(preprocessing_mecab(SAMPLE_TEXT)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SwoCc4eYL5Q4"},"source":["total_df['tweet'] = total_df['tweet'].apply(lambda x: preprocessing_mecab(x))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mv2Le5qUL5Q4"},"source":["total_df['tweet']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6U0lFfLXL5Q4"},"source":["# # Find topics"]},{"cell_type":"code","metadata":{"id":"LTLzMFUBL5Q5"},"source":["data_lemmatized = total_df['tweet'].tolist()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A6AW2u_qL5Q5"},"source":["id2word = corpora.Dictionary(data_lemmatized)\n","texts = data_lemmatized\n","\n","corpus = [id2word.doc2bow(text) for text in texts]\n","print(corpus[:1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cN6-VLB-L5Q5"},"source":["[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c3xWn1SYL5Q6"},"source":["lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n","                                           id2word=id2word,\n","                                           num_topics=20,\n","                                           random_state=100,\n","                                           update_every=1,\n","                                           chunksize=100,\n","                                           passes=2,\n","                                           alpha='auto',\n","                                           per_word_topics=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"7fnJKsjYL5Q6"},"source":["pprint(lda_model.print_topics())\n","doc_lda = lda_model[corpus]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J8X1QdbLL5Q7"},"source":["# Compute Perplexity\n","print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n","\n","# Compute Coherence Score\n","coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n","coherence_lda = coherence_model_lda.get_coherence()\n","print('\\nCoherence Score: ', coherence_lda)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sVj0c2KYL5Q7"},"source":["# Visualization"]},{"cell_type":"code","metadata":{"id":"pcl9EBeGL5Q8"},"source":["# Visualize the topics\n","pyLDAvis.enable_notebook()\n","vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n","vis"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-uYO6CewL5Q8"},"source":["# Choose the number of topics"]},{"cell_type":"code","metadata":{"id":"sCB48HmmL5Q8"},"source":["from tqdm import tqdm"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"15F-Pim4L5Q8"},"source":["def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n","    coherence_values = []\n","    model_list = []\n","    for num_topics in tqdm(range(start, limit, step)):\n","        model =  gensim.models.ldamodel.LdaModel(corpus=corpus,\n","                                           id2word=id2word,\n","                                           num_topics=num_topics,\n","                                           random_state=100,\n","                                           update_every=1,\n","                                           chunksize=100,\n","                                           passes=2,\n","                                           alpha='auto',\n","                                           per_word_topics=True)\n","\n","        model_list.append(model)\n","        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n","        coherence_values.append(coherencemodel.get_coherence())\n","    return model_list, coherence_values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bZdsHvU_L5Q9"},"source":["model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=data_lemmatized, start=2, limit=35, step=6)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-4i3Eq3qL5Q9"},"source":["limit=35; start=2; step=6;\n","x = range(start, limit, step)\n","plt.plot(x, coherence_values)\n","plt.xlabel(\"Num Topics\")\n","plt.ylabel(\"Coherence score\")\n","plt.legend((\"coherence_values\"), loc='best')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g232GiiSL5Q-"},"source":["# Finding the dominant topic in each sentence"]},{"cell_type":"code","metadata":{"id":"EBtR9a5YL5Q-"},"source":["optimal_model = model_list[5]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kEqwwwMVL5Q-"},"source":["optimal_model.per_word_topics = False"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AYELdP2GL5Q-"},"source":["# Compute Perplexity\n","print('\\nPerplexity: ', optimal_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n","\n","# Compute Coherence Score\n","coherence_model_lda = CoherenceModel(model=optimal_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n","coherence_lda = coherence_model_lda.get_coherence()\n","print('\\nCoherence Score: ', coherence_lda)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PQ03fhC_L5Q-"},"source":["def format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=None):\n","    sent_topics_df = pd.DataFrame()\n","\n","    # Get main topic in each document\n","    for i, row in enumerate(tqdm(ldamodel[corpus])):\n","        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n","        # Get the Dominant topic, Perc Contribution and Keywords for each document\n","        for j, (topic_num, prop_topic) in enumerate(row):\n","            if j == 0:  # => dominant topic\n","                wp = ldamodel.show_topic(topic_num)\n","                topic_keywords = \", \".join([word for word, prop in wp])\n","                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n","            else:\n","                break\n","    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n","\n","    # Add original text to the end of the output\n","    contents = pd.Series(texts)\n","    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n","    return(sent_topics_df)\n","\n","\n","df_topic_sents_keywords = format_topics_sentences(ldamodel=optimal_model, corpus=corpus, texts=total_df['tweet'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JEDsaRxOL5Q_"},"source":["df_dominant_topic = df_topic_sents_keywords.reset_index()\n","df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"Ltk9s-LzL5Q_"},"source":["df_dominant_topic.head(50)"],"execution_count":null,"outputs":[]}]}