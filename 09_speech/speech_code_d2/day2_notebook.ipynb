아래 파이썬 코드를 실행했더니 결과가 torch.Size([16, 500, 29]) 16 가 나와야 하는데 torch.Size([16, 500, 256]) 16 이렇게 나왔어. 
DeepSpeech2Model의 코드를 수정해야 하는데 어디가 문제인지 찾아서 수정해줘.

import torch
import torch.nn as nn


class CTCHead(nn.Module):

    def __init__(self, feature_dim: int, vocab_size: int) -> None:
        super().__init__()
        self.feature_dim = feature_dim
        self.vocab_size = vocab_size

        self.bn = nn.BatchNorm1d(feature_dim)
        self.fc = nn.Linear(feature_dim, vocab_size)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        :param x:       (batch_size, seq_len, feature_dim)
        :return:
                output: (batch_size, seq_len, vocab_size)
        """
        b, s, d = x.shape
        assert d == self.feature_dim

        x = x.view(b * s, d)
        x = self.bn(x)  # (b * s, d)
        x = self.fc(x)  # (b * s, d) -> (b * s, v)
        x = x.view(b, s, self.vocab_size)  # (b, s, v)
        return x


from typing import Tuple
import torch
import torch.nn as nn
from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence


class GRULayer(nn.Module):

    def __init__(self, input_dim: int, hidden_dim: int):
        super().__init__()

        self.input_dim = input_dim
        self.hidden_dim = hidden_dim

        self.bn = nn.BatchNorm1d(input_dim)
        self.rnn = nn.GRU(input_dim, hidden_dim, num_layers=1, bias=True,
                          batch_first=True, bidirectional=True)

    def forward(self, x: torch.Tensor, lengths: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        :param x:                   (batch_size, seq_len, input_dim)
        :param lengths:             (batch_size,)                       long
        :return:
                output:             (batch_size, seq_len, hidden_dim)
                state:              (2, batch_size, hidden_dim) GRU states: 2 is for bidirectional
        """
        b, s, d = x.shape
        assert d == self.input_dim

        x = x.view(b * s, d)
        x = self.bn(x)
        x = x.view(b, s, d)

        x = pack_padded_sequence(x, lengths.cpu().int(), batch_first=True, enforce_sorted=False)
        x, state = self.rnn(x)  # (b, s, 2 * h), (2, b, h)
        x, _ = pad_packed_sequence(x, batch_first=True)

        # handle bidirectional
        x = x.view(b, s, 2, -1)  # (b, s, 2, h)
        x = torch.sum(x, dim=2)  # (b, s, h)
        return x, state


class GRUBody(nn.Module):

    def __init__(self, num_layers: int, input_dim: int, hidden_dim: int):
        super().__init__()
        self.num_layers = num_layers
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim

        rnn_layers = []
        rnn_dim = input_dim
        for i in range(num_layers):
            rnn_layers.append(GRULayer(rnn_dim, hidden_dim))
            rnn_dim = hidden_dim

        self.rnn_layers = nn.ModuleList(rnn_layers)

    def forward(self, x: torch.Tensor, lengths: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:

        states = []
        for i in range(self.num_layers):
            x, s = self.rnn_layers[i](x, lengths)
            states.append(s)

        states = torch.cat(states, dim=0)  # (2 * num_layers, b, h)
        return x, states

from typing import Optional, Tuple
import torch
import torch.nn as nn


@torch.no_grad()
def make_mask_by_length(lengths: torch.Tensor, max_length: Optional[int] = None) -> torch.Tensor:
    assert lengths.ndim == 1
    batch_size = lengths.shape[0]

    if max_length is None:
        max_length = lengths.max().item()

    device = lengths.device

    seq_range = torch.arange(0, max_length, dtype=torch.long, device=device)  # (s,)
    seq_range = seq_range.unsqueeze(0).expand(batch_size, max_length)  # (b, s)
    seq_length = lengths.unsqueeze(1).expand(batch_size, max_length)  # (b, s)
    mask = torch.less(seq_range, seq_length)  # (b, s)
    return mask


@torch.no_grad()
def fix_lengths_after_conv(lengths: torch.Tensor, kernel_size: int, stride: int, padding: int) -> torch.Tensor:
    new_lengths = torch.floor((lengths + 2 * padding - (kernel_size - 1) - 1) / stride + 1)
    return new_lengths.long()


class ConvSubsampling4x2(nn.Module):

    def __init__(self,
                 feature_dim: int,
                 out_dim: Optional[int]) -> None:
        super().__init__()
        assert feature_dim % 4 == 0
        self.feature_dim = feature_dim

        self.conv1 = nn.Conv2d(1, 32, kernel_size=(41, 11), stride=(2, 2), padding=(20, 5), bias=False)
        self.bn1 = nn.BatchNorm2d(32)
        self.act1 = nn.Hardtanh(0, 20, inplace=True)
        self.conv2 = nn.Conv2d(32, 32, kernel_size=(21, 11), stride=(2, 1), padding=(10, 5), bias=False)
        self.bn2 = nn.BatchNorm2d(32)
        self.act2 = nn.Hardtanh(0, 20, inplace=True)

        # if feature dimension is 80, it is converted as:
        # (channel, dim)
        # conv1: (1, 80) -> (32, 40)
        # conv2: (32, 40) -> (32, 40)
        out_feature_dim = 32 * feature_dim // 2

        # this linear is not in DS2 paper, but it is common practice in recent works.
        if out_dim is not None:
            self.out_dim = out_dim
            self.linear = nn.Linear(out_feature_dim, out_dim, bias=False)  # will be followed by BN
        else:
            self.out_dim = out_feature_dim
            self.linear = None

    def forward(self, x: torch.Tensor, lengths: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        :param x:               (batch_size, seq_length, feature_dim)       float
        :param lengths:         (batch_size,)                               long
        :return:
                output:         (batch_size, seq_length // 4, out_dim)
                out_lengths:    (batch_size,)                               long
        """
        if x.ndim == 3:
            x = x.unsqueeze(1)

        batch_size = x.shape[0]

        # we keep mask out invalid parts to ensure that the result does not depends on the sequence length.

        # first convolution input should be already zero-ed for invalid regions.
        x = self.conv1(x)  # (b, 1, s, d) -> (b, 32, s//2, d//2)
        lengths = fix_lengths_after_conv(lengths, 41, 2, 20)
        mask = make_mask_by_length(lengths)  # (b, s//2)
        x = x * mask.view(batch_size, 1, -1, 1)

        x = self.bn1(x)
        x = x * mask.view(batch_size, 1, -1, 1)  # bn does not change mask length
        x = self.act1(x)  # HardTanh keep 0 as 0

        x = self.conv2(x)  # (b, 32, s//2, d//2) -> (b, 32, s//4, d//2)
        lengths = fix_lengths_after_conv(lengths, 21, 2, 10)
        mask = make_mask_by_length(lengths)  # (b, s//4)
        x = x * mask.view(batch_size, 1, -1, 1)

        x = self.bn2(x)
        x = x * mask.view(batch_size, 1, -1, 1)  # bn does not change mask length
        x = self.act2(x)  # HardTanh keep 0 as 0

        _, c, t, f = x.shape
        # aggregate feature for each time-step
        x = x.transpose(1, 2).contiguous().view(batch_size, t, c * f)
        if self.linear is not None:
            x = self.linear(x)  # (b, s//4, 32 * d//2) -> (b, s//4, out_dim)
            x = x * mask.view(batch_size, -1, 1)
        return x, lengths



from typing import Tuple, Optional
import torch
import torch.nn as nn

from model.subsampling import ConvSubsampling4x2
from model.rnn import GRUBody
from model.head import CTCHead


class DeepSpeech2Model(nn.Module):

    def __init__(self,
                 feature_dim: int,
                 num_layers: int,
                 hidden_dim: int,
                 vocab_size: int) -> None:
        super().__init__()
        # Define DeepSpeech2 Layers here
        # with 'ConvSubsampling4x2', 'GRUBody', 'CTCHead' module
        self.conv = ConvSubsampling4x2(feature_dim, hidden_dim)
        self.rnn = GRUBody(num_layers, hidden_dim, hidden_dim)
        self.head = CTCHead(hidden_dim, vocab_size)

    def forward(self,
                x: torch.Tensor,
                lengths: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        :param x:               (batch_size, seq_length, feature_dim)       float
        :param lengths:         (batch_size,)                               long
        :return:
                output:         (batch_size, seq_length // 4, vocab_size)
                out_lengths:    (batch_size,)                               long
        """
        if lengths is None:
            max_seq_length = x.shape[1]
            lengths = torch.full((x.shape[0],), max_seq_length, dtype=torch.long, device=x.device)
        # Connect layers to produce proper output from input spectrogram
        # input x -> Conv -> GRU -> CTCHead -> head_output
        x, lengths = self.conv(x, lengths)
        x, _ = self.rnn(x, lengths)
        output = self.head(x)

        return x, lengths



import torch
from model.deepspeech2 import DeepSpeech2Model

#Define parameters
n_mel = 80
n_vocab = 29  # graphemes(28) + <BLK>
batch_size = 16
max_frame_length = 2000
n_layers = 4
h_dim = 256

# Define DeepSpeech2Model
speech_model = DeepSpeech2Model(n_mel, n_layers, h_dim, n_vocab)

# test your DeepSpeech2Model
test_input = torch.rand((batch_size, max_frame_length, n_mel))

model_output, length = speech_model(test_input)
print(model_output.shape, len(length))  #should be "[16, 500, 29] 16"